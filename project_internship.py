# -*- coding: utf-8 -*-
"""Project-Internship.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yfHEYTHvPLmKYGtjmB6Z9GJehcEMTA73

Import all necessary Libraries
"""

import numpy as  np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""Step01 - DATA COLLECTION"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/CrimeProjectInternship/crime_dataset_india.csv')

df.head(n=10)

df.dtypes

df.columns

df.shape

df.describe()

"""STEP02 - DATA PRE-PROCESSING"""

# Drop rows with missing critical fields
df = df.dropna(subset=["Time of Occurrence", "Crime Description"])

# Fill "Date Case Closed" with placeholder
df["Date Case Closed"] = df["Date Case Closed"].fillna("Not Closed")

df.isnull().sum()

df.drop(columns=['Weapon Used'], inplace=True)

df.head(n=10)

# Convert date columns to datetime
date_cols = ["Date Reported", "Date of Occurrence", "Date Case Closed"]
for col in date_cols:
    df[col] = pd.to_datetime(df[col], errors='coerce')

# Extract features from datetime
df["Year"] = df["Date of Occurrence"].dt.year
df["Month"] = df["Date of Occurrence"].dt.month
df["Weekday"] = df["Date of Occurrence"].dt.day_name()

# Convert "Time of Occurrence" to "Time Category"
def func(row):
    try:
        time_str = row.split()[1]
        hour = int(time_str.split(":")[0])
        return "Night" if (hour >= 20 or hour < 5) else "Day"
    except:
        return "Unknown"
df["Time Category"] = df["Time of Occurrence"].apply(func)

# Assign severity scores
severity_mapping = {
    "HOMICIDE": 5,
    "ASSAULT": 4,
    "KIDNAPPING": 4,
    "BURGLARY": 3,
    "VANDALISM": 2,
    "IDENTITY THEFT": 2,
    "FRAUD": 3,
    "THEFT": 2,
    "ROBBERY": 4,
    "OTHER": 1
}
df["Severity Score"] = df["Crime Description"].apply(lambda x: severity_mapping.get(x.upper(), 1))

# Binary encode "Case Closed"
df["Case Closed Binary"] = df["Case Closed"].apply(lambda x: 1 if str(x).strip().upper() == "YES" else 0)

# Flag for high-risk crimes during night
df["High Risk Night"] = df.apply(lambda row: 1 if row["Severity Score"] >= 4 and row["Time Category"] == "Night" else 0, axis=1)

# Create Risk Score (scaled)
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[["Victim Age", "Police Deployed", "Severity Score"]] = scaler.fit_transform(
    df[["Victim Age", "Police Deployed", "Severity Score"]]
)

df["Risk Score"] = df["Victim Age"] * 0.1 + df["Police Deployed"] * 0.3 + df["Severity Score"] * 0.6

df.copy()

df['Crime Description'].value_counts()

df['Month'].value_counts().sort_values()

df['Time of Occurrence'].value_counts()

df['City'].value_counts()

df['Victim Gender'].value_counts()

df.columns

# Time Category
sns.set(style="darkgrid")
plt.figure(figsize=(6, 4))
sns.countplot(data=df, x="Time Category", palette="coolwarm")
plt.title("Crime Count: Day vs Night")
plt.xlabel("Time of Day")
plt.ylabel("Number of Crimes")
plt.tight_layout()
plt.show()

# Top 10 Crime Types
plt.figure(figsize=(10, 5))
top_crimes = df["Crime Description"].value_counts().head(10)
sns.barplot(x=top_crimes.values, y=top_crimes.index, palette="viridis")
plt.title("Top 10 Crime Types")
plt.xlabel("Number of Cases")
plt.ylabel("Crime Type")
plt.tight_layout()
plt.show()

# Crime Frequency by Weekday
plt.figure(figsize=(8, 4))
sns.countplot(data=df, x="Weekday", order=[
    "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"],
    palette="Set2")
plt.title("Crimes by Day of the Week")
plt.xlabel("Weekday")
plt.ylabel("Count")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Top 10 Cities with High Risk Night Crimes
plt.figure(figsize=(10, 6))
top_cities_hrn = df[df["High Risk Night"] == 1]["City"].value_counts().head(10)
sns.barplot(x=top_cities_hrn.values, y=top_cities_hrn.index, palette="rocket")
plt.title("Top 10 Cities with High Risk Crimes at Night")
plt.xlabel("Number of High Risk Night Crimes")
plt.ylabel("City")
plt.tight_layout()
plt.show()

# Distribution of Risk Scores
plt.figure(figsize=(8, 4))
sns.histplot(df["Risk Score"], bins=20, kde=True, color="purple")
plt.title("Distribution of Calculated Risk Scores")
plt.xlabel("Risk Score")
plt.ylabel("Crime Frequency")
plt.tight_layout()
plt.show()

"""STEP03 - CREATE CRIME HEATMAPS USING GIS TOOLS"""

df2 = pd.read_csv("/content/in.csv")
df2.head(n=10)

df2.isnull().sum()

df2.drop(columns=['country','iso2','admin_name','capital','population','population_proper'], inplace=True)

df2.head(n=10)

# Clean city names for matching
df2["City_clean"] = df2["city"].str.strip().str.upper()
df["City_clean"] = df["City"].str.strip().str.upper()

# Merge
final_df = pd.merge(df, df2, on="City_clean", how="left")

crime_df = pd.read_csv("/content/final_df.csv")

crime_df = crime_df.dropna(subset=["lat", "lng"])
crime_df.head(n=3)

# extract unique Cities
cities = crime_df["City_clean"].dropna().unique().tolist()
print("Cities found:", cities)

crime_df.shape

# export crimedf to csv
crime_df.to_csv("crime_df.csv", index=False)

crime_df['City'].value_counts()

crime_df['Risk Score'].value_counts()

"""Normalise Risk Scores"""

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
crime_df['normalized_risk'] = scaler.fit_transform(crime_df[['Risk Score']])

"""Assign Risk Levels / classes"""

def func(score):
    if score >= 0.2:
        return 'red'
    elif score > 0.1 and score < 0.2:
        return 'yellow'
    else:
        return 'green'

crime_df['risk_levels'] = crime_df['normalized_risk'].apply(func)

"""MODEL TRAINING AND EVALUATION PART"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

crime_df.columns

# crime_df.drop(columns=['Time Category', 'Case Closed Binary', 'City_clean', 'Case Closed', 'Date Case Closed', 'Police Deployed', 'Victim Gender', 'Victim Age', 'Date Reported', 'Report Number', 'Crime Code', 'Year', 'Month', 'Weekday'], inplace=True)

# crime_df.drop(columns=['city'], inplace=True)

bins = [0.0, 0.1, 0.2, 1.0]
labels = ['Green', 'Yellow', 'Red']
crime_df['risk_levels'] = pd.cut(crime_df['Risk Score'], bins=bins, labels=labels, include_lowest=True)

X = crime_df.select_dtypes(include=['int64', 'float64']).drop(columns=['Risk Score'])
y = crime_df['risk_levels']

crime_df = crime_df.dropna(subset=['risk_levels'])

crime_df.head(n=10)

"""Model01 - LOGISTIC REGRESSION"""

# Scale numerical features
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score, KFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
LR = LogisticRegression(max_iter=1000,class_weight='balanced')
LR_scores = cross_val_score(LR, X_scaled, y, cv=kf, scoring='accuracy')

{
    'Model': 'Logistic Regression',
    'Accuracy Mean': np.mean(LR_scores),
}

"""Model02 - SUPPORT VECTOR CLASSIFIER"""

# Scale numerical features
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
Svc = SVC(class_weight='balanced',probability=True)
Svc_scores = cross_val_score(Svc, X_scaled, y, cv=kf, scoring='accuracy')

{
    'Model': 'Support Vector Classifier',
    'Accuracy Mean': np.mean(Svc_scores)
}

"""MODEL03 - XGBOOST CLASSIFIER"""

# Scale numerical features
from xgboost import XGBClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

le = LabelEncoder()
y_encoded = le.fit_transform(y)

# 5-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True)
XGB = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss',class_weight='balanced')
XGB_scores = cross_val_score(XGB, X_scaled, y_encoded, cv=kf, scoring='accuracy')

{
    'Model': 'XGBOOST Classifier',
    'Accuracy Mean': np.mean(XGB_scores)
}

"""MODEL04 - LIGHTGBM CLASSIFIER"""

# Scale numerical features
from lightgbm import LGBMClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True)
LGBM = LGBMClassifier(random_state=42,class_weight='balanced')
LGBM_scores = cross_val_score(LGBM, X_scaled, y, cv=kf, scoring='accuracy')

{
    'Model': 'LightGBM Classifier',
    'Accuracy Mean': np.mean(LGBM_scores)
}

"""MODEL05 - MLP CLASSIFIER"""

# Scale numerical features
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import cross_val_score, KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 5-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
MLP = MLPClassifier(max_iter=15)
MLP_scores = cross_val_score(MLP, X_scaled, y, cv=kf, scoring='accuracy')

{
    'Model': 'MLP Classifier',
    'Accuracy Mean': np.mean(MLP_scores)
}

# compare all model results using line plot
models = ['Logistic Regression', 'Support Vector Classifier', 'XGBOOST Classifier', 'LightGBM Classifier', 'MLP Classifier']
accuracy_means = [np.mean(LR_scores), np.mean(Svc_scores), np.mean(XGB_scores), np.mean(LGBM_scores), np.mean(MLP_scores)]

plt.figure(figsize=(10, 6))
plt.plot(models, accuracy_means, marker='o', linestyle='-', color='b')
plt.title('Model Accuracy Comparison')
plt.xlabel('Models')
plt.ylabel('Accuracy Mean')
plt.grid(True)
plt.xticks(rotation=45)
plt.yticks(rotation=45)
plt.tight_layout()
plt.show()

"""Plot Risk Hotspots for Each City - Creating Crime Heatmaps"""

!pip install folium

# Get list of unique cities
cities = crime_df['City'].unique()
cities

import folium
from IPython.display import display

city_maps = {}

for city in cities:
    city_data = crime_df[crime_df['City'] == city]

    m = folium.Map(location=[city_data['lat'].mean(), city_data['lng'].mean()], zoom_start=10)
    for _, row in city_data.iterrows():
        folium.CircleMarker(
            location=(row['lat'], row['lng']),
            radius=5,
            color=row['risk_levels'],
            fill=True,
            fill_color=row['risk_levels'],
            fill_opacity=0.7,
            popup=f"""
            City: {row['City']}<br>
            Crime Description: {row['Crime Description']}<br>
            Time of Occurrence: {row['Time of Occurrence']}<br>
            Latitude: {round(row['lat'], 4)}<br>
            Longitude: {round(row['lng'], 4)}<br>
            Risk Score: {round(row['Risk Score'], 2)}
            """
        ).add_to(m)

    city_maps[city] = m

# Display maps for each city
for city, map in city_maps.items():
    print(f"Crime Hotspots Created for--- {city}")

crime_df['risk_levels'].value_counts()

display(city_maps['Mumbai'])

display(city_maps['Delhi'])

display(city_maps['Indore'])

display(city_maps['Ahmedabad'])

display(city_maps['Lucknow'])

display(city_maps['Chennai'])

display(city_maps['Patna'])

display(city_maps['Meerut'])

display(city_maps['Jaipur'])

display(city_maps['Bangalore'])

display(city_maps['Pune'])

display(city_maps['Kanpur'])

print(city_data[['lat', 'lng']].describe())
print(city_data[['lat', 'lng']].drop_duplicates().shape)

city_data = city_data.drop_duplicates(subset=['lat', 'lng'])

import folium
from folium.plugins import HeatMap
from IPython.display import display
import numpy as np

city_heatmaps = {}

for city in cities:
    city_data = crime_df[crime_df['City'] == city].dropna(subset=['lat', 'lng', 'Risk Score'])
    city_data['lat'] += np.random.normal(0, 0.001, size=len(city_data))
    city_data['lng'] += np.random.normal(0, 0.001, size=len(city_data))

    # Create map centered on city
    m = folium.Map(location=[city_data['lat'].mean(), city_data['lng'].mean()], zoom_start=12)

    # Add heatmap layer
    heat_data = city_data[['lat', 'lng', 'Risk Score']].values.tolist()
    HeatMap(heat_data, radius=15, blur=10).add_to(m)

    for _, row in city_data.iterrows():
        popup_text = f"""
        <b>Crime:</b> {row['Crime Description']}<br>
        <b>Time:</b> {row['Time of Occurrence']}<br>
        <b>Lat, Lng:</b> {round(row['lat'], 4)}, {round(row['lng'], 4)}<br>
        <b>Risk Score:</b> {round(row['Risk Score'], 2)}
        """
        folium.CircleMarker(
            location=(row['lat'], row['lng']),
            radius=4,
            color='red',
            fill=True,
            fill_color='red',
            fill_opacity=0.6,
            popup=folium.Popup(popup_text, max_width=300)
        ).add_to(m)

    city_heatmaps[city] = m

display(city_heatmaps['Mumbai'])

display(city_heatmaps['Delhi'])

display(city_heatmaps['Indore'])

display(city_heatmaps['Ahmedabad'])

"""Creating Road Network Graph for Each City Based on Crime Hotspots"""

!pip install osmnx

!pip install networkx

"""Use OpenStreetMap (OSM) data to extract road networks"""

import osmnx as ox
import networkx as nx

"""Calculate Road Network Analysis for Each City"""

cities = crime_df['City'].unique()
city_graphs = {}
for city in cities:
   print(f"Extracting road network data for--- {city}")
   G = ox.graph_from_place(city + ', India', network_type='drive')
   city_graphs[city] = G

crime_df.columns

import osmnx as ox
import folium
import numpy as np

def create_city_network_data(city_name, crime_data, high_risk_threshold=0.2):
    # Filter data for city
    city_crime_df = crime_df[crime_df['City'] == city_name].dropna(subset=['lat', 'lng', 'Risk Score'])
    if city_crime_df.empty:
        print(f"No data for {city_name}")
        return None, None, None

    # Generate road graph
    G = ox.graph_from_place(f"{city_name}, India", network_type='walk')

    # Create folium map centered on city
    lat_center = city_crime_df['lat'].mean()
    lng_center = city_crime_df['lng'].mean()
    fmap = folium.Map(location=[lat_center, lng_center], zoom_start=13, tiles='CartoDB dark_matter')

    # Plot high-risk crime hotspots as red circles
    high_risk_crimes = city_crime_df[city_crime_df['Risk Score'] >= high_risk_threshold]
    for _, row in high_risk_crimes.iterrows():
        folium.CircleMarker(
            location=(row['lat'], row['lng']),
            radius=5,
            color='red',
            fill=True,
            fill_opacity=0.7,
            popup=f"Risk Score: {row['Risk Score']:.2f}"
        ).add_to(fmap)

    return G, city_crime_df, fmap

"""Add Dijkstra Routing (Shortest & Safest Paths)"""

import networkx as nx
from folium import PolyLine

def dijkstra_algorithm(G, fmap, crime_data, start_points=None, end_points=None, city="Unknown", high_risk_threshold=0.2):
    if crime_df.empty:
        print(f"No crime data to process routes in {city}")
        return fmap

    # Define source and destination nodes
    if start_points is None:
        start_points = (crime_df.iloc[0]['lat'], crime_df.iloc[0]['lng'])
    if end_points is None:
        end_points = (crime_df.iloc[-1]['lat'], crime_df.iloc[-1]['lng'])

    start_node = ox.distance.nearest_nodes(G, start_points[1], start_points[0])
    end_node = ox.distance.nearest_nodes(G, end_points[1], end_points[0])

    # Assign weights to edges based on crime proximity
    for u, v, k, data in G.edges(keys=True, data=True):
        data['length'] = data.get('length', 1)
        midpoint = ((G.nodes[u]['y'] + G.nodes[v]['y']) / 2, (G.nodes[u]['x'] + G.nodes[v]['x']) / 2)
        nearby_crimes = crime_df[
            ((crime_df['lat'] - midpoint[0]).abs() < 0.002) &
            ((crime_df['lng'] - midpoint[1]).abs() < 0.002)
        ]
        data['risk'] = 5 if not nearby_crimes.empty and nearby_crimes['Risk Score'].mean() > high_risk_threshold else 1

    # Calculate shortest path using Dijkstra's algorithm
    try:
        shortest_path = nx.shortest_path(G, start_node, end_node, weight='length')
        shortest_distance = nx.shortest_path_length(G, start_node, end_node, weight='length')
        points_short = [(G.nodes[n]['y'], G.nodes[n]['x']) for n in shortest_path]
        PolyLine(points_short, color='red', dash_array='5,5', weight=3, popup=f"Shortest Path: {shortest_distance:.2f} meters").add_to(fmap)
        folium.Marker(points_short[0], icon=folium.Icon(color='blue'),
                      popup='Start Point (Shortest)').add_to(fmap)
        folium.Marker(points_short[-1], icon=folium.Icon(color='purple'),
                      popup='End Point (Shortest)').add_to(fmap)
    except Exception as e:
        print(f"Could not compute shortest path in {city}: {e}")

    # Add safest path (based on risk)
    try:
        safest_path = nx.shortest_path(G, start_node, end_node, weight='risk')
        safest_distance = nx.shortest_path_length(G, start_node, end_node, weight='risk')
        points_safe = [(G.nodes[n]['y'], G.nodes[n]['x']) for n in safest_path]
        PolyLine(points_safe, color='green', weight=6, popup=f"Safest Path: {safest_distance:.2f} units").add_to(fmap)
        folium.Marker(points_safe[0], icon=folium.Icon(color='blue'),
                      popup='Start Point (Safest)').add_to(fmap)
        folium.Marker(points_safe[-1], icon=folium.Icon(color='purple'),
                      popup='End Point (Shortest)').add_to(fmap)
    except Exception as e:
        print(f"Could not compute safest path in {city}: {e}")

    return fmap

unique_cities = crime_df['City'].dropna().unique()
city_final_maps = {}

for city in unique_cities:
    print(f"\nProcessing {city}...")
    G, city_data, base_map = create_city_network_data(city, crime_df)
    if G is None or base_map is None:
        continue

    final_map = dijkstra_algorithm(G, base_map, city_data, city=city)
    final_map.save(f"{city}_safest_route_map.html")
    city_final_maps[city] = final_map

# Usage
city = "Delhi"
G, city_data, base_map = create_city_network_data(city, crime_df)
final_map = dijkstra_algorithm(G, base_map, city_data, city=city)
# now display on folium map
final_map

# Usage
city = "Pune"
G, city_data, base_map = create_city_network_data(city, crime_df)
final_map = dijkstra_algorithm(G, base_map, city_data, city=city)
# now display on folium map
final_map

# Usage
city = "Lucknow"
G, city_data, base_map = create_city_network_data(city, crime_df)
final_map = dijkstra_algorithm(G, base_map, city_data, city=city)
# now display on folium map
final_map